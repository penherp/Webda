{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyONwC1Ajzyh/fMj+Ftn6MOt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AA86S5Ub74t0","executionInfo":{"status":"ok","timestamp":1741625089598,"user_tz":-330,"elapsed":3642,"user":{"displayName":"Joel Chinta","userId":"00861839643618150718"}},"outputId":"053fc5ca-c1e0-4198-d3a3-191e4ba5fe45"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset Sample (first 5 rows):\n","   ID   TV  Radio  Newspaper  Sales\n","0   1  112     46         32    108\n","1   2  280     13         37    121\n","2   3  116     54         12    197\n","3   4   81     31         30    142\n","4   5  198     70         46    112\n","\n","Model Coefficients:\n","TV coefficient: 0.0990\n","Radio coefficient: 0.1756\n","Newspaper coefficient: 0.1176\n","Intercept: 144.1505\n","\n","Model Performance:\n","Mean Squared Error: 5218.3077\n","R-squared: -0.0001\n"]}],"source":["#1. Linear Regression - Sales Dataset\n","\n","#(Slip No. 1, 12 - Identical)\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# Set random seed for reproducibility\n","np.random.seed(42)\n","\n","# Create the 'sales' dataset with 500 random entries\n","data = {\n","    'ID': np.arange(1, 501),\n","    'TV': np.random.randint(10, 300, 500),\n","    'Radio': np.random.randint(5, 100, 500),\n","    'Newspaper': np.random.randint(0, 50, 500),\n","    'Sales': np.random.randint(50, 300, 500)\n","}\n","df = pd.DataFrame(data)\n","print(\"Dataset Sample (first 5 rows):\")\n","print(df.head())\n","\n","# Define independent (X) and target (y) variables\n","X = df[['TV', 'Radio', 'Newspaper']]  # Independent variables\n","y = df['Sales']  # Target variable\n","\n","# Split into training and testing sets (70% train, 30% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Build and train the linear regression model\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Print model coefficients and performance metrics\n","print(\"\\nModel Coefficients:\")\n","print(f\"TV coefficient: {model.coef_[0]:.4f}\")\n","print(f\"Radio coefficient: {model.coef_[1]:.4f}\")\n","print(f\"Newspaper coefficient: {model.coef_[2]:.4f}\")\n","print(f\"Intercept: {model.intercept_:.4f}\")\n","\n","print(\"\\nModel Performance:\")\n","print(f\"Mean Squared Error: {mse:.4f}\")\n","print(f\"R-squared: {r2:.4f}\")"]},{"cell_type":"code","source":["#2. Linear Regression - Real Estate Dataset\n","\n","#(Slip No. 4, 15 - Identical)\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","\n","# Set random seed for reproducibility\n","np.random.seed(42)\n","\n","# Create the 'realestate' dataset with 500 random entries\n","data = {\n","    'ID': np.arange(1, 501),\n","    'flat': np.random.randint(10, 100, 500),\n","    'houses': np.random.randint(5, 50, 500),\n","    'purchases': np.random.randint(50, 200, 500)\n","}\n","df = pd.DataFrame(data)\n","\n","# Define independent (X) and target (y) variables\n","X = df[['flat', 'houses']]  # Independent variables\n","y = df['purchases']  # Target variable\n","\n","# Split into training and testing sets (70% train, 30% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Build and train the linear regression model\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = model.predict(X_test)\n","\n","# Print model coefficients\n","print(\"\\nModel Coefficients:\")\n","print(f\"Flat coefficient: {model.coef_[0]:.4f}\")\n","print(f\"Houses coefficient: {model.coef_[1]:.4f}\")\n","print(f\"Intercept: {model.intercept_:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aw_0VyFu8gTr","executionInfo":{"status":"ok","timestamp":1741625130235,"user_tz":-330,"elapsed":457,"user":{"displayName":"Joel Chinta","userId":"00861839643618150718"}},"outputId":"b1c70a60-21fc-4dbb-f6b8-bbfb5c9a973e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Model Coefficients:\n","Flat coefficient: 0.0684\n","Houses coefficient: 0.1469\n","Intercept: 115.3210\n"]}]},{"cell_type":"code","source":["#3. Logistic Regression - User Dataset\n","\n","#(Slip No. 2, 18 - Identical)\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# Create 'User' dataset with 500 entries\n","np.random.seed(42)\n","data = {\n","    'User ID': np.arange(1, 501),\n","    'Gender': np.random.choice(['Male', 'Female'], 500),\n","    'Age': np.random.randint(18, 60, 500),\n","    'EstimatedSalary': np.random.randint(20000, 120000, 500),\n","    'Purchased': np.random.choice([0, 1], 500)\n","}\n","df = pd.DataFrame(data)\n","\n","# Convert categorical column to numeric\n","encoder = LabelEncoder()\n","df['Gender'] = encoder.fit_transform(df['Gender'])\n","\n","# Define independent (X) and target (y) variables\n","X = df[['Gender', 'Age', 'EstimatedSalary']]\n","y = df['Purchased']\n","\n","# Split into training and testing sets (70% train, 30% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Build and train the logistic regression model\n","model = LogisticRegression()\n","model.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = model.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Model Accuracy: {accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FAT-Gqdf8qLK","executionInfo":{"status":"ok","timestamp":1741625186163,"user_tz":-330,"elapsed":396,"user":{"displayName":"Joel Chinta","userId":"00861839643618150718"}},"outputId":"6c4d54e2-f452-4342-8c65-820ed4db715c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 0.4533\n"]}]},{"cell_type":"code","source":["#4. Apriori Algorithm - Market Basket Analysis\n","\n","#(Slip No. 5, 7, 9, 10, 13, 16, 19, 20 - Identical)\n","\n","import pandas as pd\n","from mlxtend.preprocessing import TransactionEncoder\n","from mlxtend.frequent_patterns import apriori, association_rules\n","\n","# Define the dataset\n","dataset = [\n","    ['Bread', 'Milk'],\n","    ['Bread', 'Diaper', 'Beer', 'Eggs'],\n","    ['Milk', 'Diaper', 'Beer', 'Coke'],\n","    ['Bread', 'Milk', 'Diaper', 'Beer'],\n","    ['Bread', 'Milk', 'Diaper', 'Coke']\n","]\n","\n","# Convert categorical values to numeric format (one-hot encoding)\n","te = TransactionEncoder()\n","te_ary = te.fit(dataset).transform(dataset)\n","df = pd.DataFrame(te_ary, columns=te.columns_)\n","\n","# Apply Apriori algorithm with different min_support values\n","min_support_values = [0.6, 0.4, 0.2]  # Testing multiple thresholds\n","for min_sup in min_support_values:\n","    frequent_itemsets = apriori(df, min_support=min_sup, use_colnames=True)\n","    print(f\"\\nFrequent itemsets with min_support={min_sup}:\")\n","    print(frequent_itemsets)\n","\n","    # Generate association rules\n","    if not frequent_itemsets.empty:\n","        rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n","        print(\"\\nAssociation Rules:\")\n","        print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UEd_kMcy86T7","executionInfo":{"status":"ok","timestamp":1741625240657,"user_tz":-330,"elapsed":424,"user":{"displayName":"Joel Chinta","userId":"00861839643618150718"}},"outputId":"a3743799-ffa4-4563-a25d-0f4d5e151d16"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Frequent itemsets with min_support=0.6:\n","   support         itemsets\n","0      0.6           (Beer)\n","1      0.8          (Bread)\n","2      0.8         (Diaper)\n","3      0.8           (Milk)\n","4      0.6   (Beer, Diaper)\n","5      0.6  (Bread, Diaper)\n","6      0.6    (Bread, Milk)\n","7      0.6   (Diaper, Milk)\n","\n","Association Rules:\n","  antecedents consequents  support  confidence    lift\n","0      (Beer)    (Diaper)      0.6        1.00  1.2500\n","1    (Diaper)      (Beer)      0.6        0.75  1.2500\n","2     (Bread)    (Diaper)      0.6        0.75  0.9375\n","3    (Diaper)     (Bread)      0.6        0.75  0.9375\n","4     (Bread)      (Milk)      0.6        0.75  0.9375\n","5      (Milk)     (Bread)      0.6        0.75  0.9375\n","6    (Diaper)      (Milk)      0.6        0.75  0.9375\n","7      (Milk)    (Diaper)      0.6        0.75  0.9375\n","\n","Frequent itemsets with min_support=0.4:\n","    support               itemsets\n","0       0.6                 (Beer)\n","1       0.8                (Bread)\n","2       0.4                 (Coke)\n","3       0.8               (Diaper)\n","4       0.8                 (Milk)\n","5       0.4          (Beer, Bread)\n","6       0.6         (Beer, Diaper)\n","7       0.4           (Beer, Milk)\n","8       0.6        (Bread, Diaper)\n","9       0.6          (Bread, Milk)\n","10      0.4         (Diaper, Coke)\n","11      0.4           (Milk, Coke)\n","12      0.6         (Diaper, Milk)\n","13      0.4  (Beer, Bread, Diaper)\n","14      0.4   (Beer, Diaper, Milk)\n","15      0.4  (Bread, Diaper, Milk)\n","16      0.4   (Milk, Diaper, Coke)\n","\n","Association Rules:\n","       antecedents     consequents  support  confidence      lift\n","0           (Beer)        (Diaper)      0.6        1.00  1.250000\n","1         (Diaper)          (Beer)      0.6        0.75  1.250000\n","2          (Bread)        (Diaper)      0.6        0.75  0.937500\n","3         (Diaper)         (Bread)      0.6        0.75  0.937500\n","4          (Bread)          (Milk)      0.6        0.75  0.937500\n","5           (Milk)         (Bread)      0.6        0.75  0.937500\n","6           (Coke)        (Diaper)      0.4        1.00  1.250000\n","7           (Coke)          (Milk)      0.4        1.00  1.250000\n","8         (Diaper)          (Milk)      0.6        0.75  0.937500\n","9           (Milk)        (Diaper)      0.6        0.75  0.937500\n","10   (Beer, Bread)        (Diaper)      0.4        1.00  1.250000\n","11    (Beer, Milk)        (Diaper)      0.4        1.00  1.250000\n","12    (Coke, Milk)        (Diaper)      0.4        1.00  1.250000\n","13  (Diaper, Coke)          (Milk)      0.4        1.00  1.250000\n","14          (Coke)  (Diaper, Milk)      0.4        1.00  1.666667\n","\n","Frequent itemsets with min_support=0.2:\n","    support                     itemsets\n","0       0.6                       (Beer)\n","1       0.8                      (Bread)\n","2       0.4                       (Coke)\n","3       0.8                     (Diaper)\n","4       0.2                       (Eggs)\n","5       0.8                       (Milk)\n","6       0.4                (Beer, Bread)\n","7       0.2                 (Beer, Coke)\n","8       0.6               (Beer, Diaper)\n","9       0.2                 (Beer, Eggs)\n","10      0.4                 (Beer, Milk)\n","11      0.2                (Bread, Coke)\n","12      0.6              (Bread, Diaper)\n","13      0.2                (Eggs, Bread)\n","14      0.6                (Bread, Milk)\n","15      0.4               (Diaper, Coke)\n","16      0.4                 (Milk, Coke)\n","17      0.2               (Eggs, Diaper)\n","18      0.6               (Diaper, Milk)\n","19      0.4        (Beer, Bread, Diaper)\n","20      0.2          (Beer, Bread, Eggs)\n","21      0.2          (Beer, Bread, Milk)\n","22      0.2         (Beer, Diaper, Coke)\n","23      0.2           (Beer, Milk, Coke)\n","24      0.2         (Beer, Eggs, Diaper)\n","25      0.4         (Beer, Diaper, Milk)\n","26      0.2        (Bread, Diaper, Coke)\n","27      0.2          (Bread, Milk, Coke)\n","28      0.2        (Eggs, Bread, Diaper)\n","29      0.4        (Bread, Diaper, Milk)\n","30      0.4         (Milk, Diaper, Coke)\n","31      0.2  (Beer, Bread, Eggs, Diaper)\n","32      0.2  (Beer, Bread, Diaper, Milk)\n","33      0.2   (Beer, Milk, Diaper, Coke)\n","34      0.2  (Bread, Milk, Diaper, Coke)\n","\n","Association Rules:\n","              antecedents            consequents  support  confidence  \\\n","0                  (Beer)               (Diaper)      0.6        1.00   \n","1                (Diaper)                 (Beer)      0.6        0.75   \n","2                  (Eggs)                 (Beer)      0.2        1.00   \n","3                 (Bread)               (Diaper)      0.6        0.75   \n","4                (Diaper)                (Bread)      0.6        0.75   \n","5                  (Eggs)                (Bread)      0.2        1.00   \n","6                 (Bread)                 (Milk)      0.6        0.75   \n","7                  (Milk)                (Bread)      0.6        0.75   \n","8                  (Coke)               (Diaper)      0.4        1.00   \n","9                  (Coke)                 (Milk)      0.4        1.00   \n","10                 (Eggs)               (Diaper)      0.2        1.00   \n","11               (Diaper)                 (Milk)      0.6        0.75   \n","12                 (Milk)               (Diaper)      0.6        0.75   \n","13          (Beer, Bread)               (Diaper)      0.4        1.00   \n","14           (Beer, Eggs)                (Bread)      0.2        1.00   \n","15          (Eggs, Bread)                 (Beer)      0.2        1.00   \n","16                 (Eggs)          (Beer, Bread)      0.2        1.00   \n","17           (Beer, Coke)               (Diaper)      0.2        1.00   \n","18           (Beer, Coke)                 (Milk)      0.2        1.00   \n","19           (Beer, Eggs)               (Diaper)      0.2        1.00   \n","20         (Eggs, Diaper)                 (Beer)      0.2        1.00   \n","21                 (Eggs)         (Beer, Diaper)      0.2        1.00   \n","22           (Beer, Milk)               (Diaper)      0.4        1.00   \n","23          (Bread, Coke)               (Diaper)      0.2        1.00   \n","24          (Bread, Coke)                 (Milk)      0.2        1.00   \n","25          (Eggs, Bread)               (Diaper)      0.2        1.00   \n","26         (Eggs, Diaper)                (Bread)      0.2        1.00   \n","27                 (Eggs)        (Bread, Diaper)      0.2        1.00   \n","28           (Coke, Milk)               (Diaper)      0.4        1.00   \n","29         (Diaper, Coke)                 (Milk)      0.4        1.00   \n","30                 (Coke)         (Diaper, Milk)      0.4        1.00   \n","31    (Beer, Bread, Eggs)               (Diaper)      0.2        1.00   \n","32   (Beer, Eggs, Diaper)                (Bread)      0.2        1.00   \n","33  (Eggs, Bread, Diaper)                 (Beer)      0.2        1.00   \n","34           (Beer, Eggs)        (Bread, Diaper)      0.2        1.00   \n","35          (Eggs, Bread)         (Beer, Diaper)      0.2        1.00   \n","36         (Eggs, Diaper)          (Beer, Bread)      0.2        1.00   \n","37                 (Eggs)  (Beer, Bread, Diaper)      0.2        1.00   \n","38    (Beer, Bread, Milk)               (Diaper)      0.2        1.00   \n","39     (Beer, Coke, Milk)               (Diaper)      0.2        1.00   \n","40   (Beer, Diaper, Coke)                 (Milk)      0.2        1.00   \n","41           (Beer, Coke)         (Diaper, Milk)      0.2        1.00   \n","42    (Bread, Coke, Milk)               (Diaper)      0.2        1.00   \n","43  (Bread, Diaper, Coke)                 (Milk)      0.2        1.00   \n","44          (Bread, Coke)         (Diaper, Milk)      0.2        1.00   \n","\n","        lift  \n","0   1.250000  \n","1   1.250000  \n","2   1.666667  \n","3   0.937500  \n","4   0.937500  \n","5   1.250000  \n","6   0.937500  \n","7   0.937500  \n","8   1.250000  \n","9   1.250000  \n","10  1.250000  \n","11  0.937500  \n","12  0.937500  \n","13  1.250000  \n","14  1.250000  \n","15  1.666667  \n","16  2.500000  \n","17  1.250000  \n","18  1.250000  \n","19  1.250000  \n","20  1.666667  \n","21  1.666667  \n","22  1.250000  \n","23  1.250000  \n","24  1.250000  \n","25  1.250000  \n","26  1.250000  \n","27  1.666667  \n","28  1.250000  \n","29  1.250000  \n","30  1.666667  \n","31  1.250000  \n","32  1.250000  \n","33  1.666667  \n","34  1.666667  \n","35  1.666667  \n","36  2.500000  \n","37  2.500000  \n","38  1.250000  \n","39  1.250000  \n","40  1.250000  \n","41  1.666667  \n","42  1.250000  \n","43  1.250000  \n","44  1.666667  \n"]}]},{"cell_type":"code","source":["#5. Text Summarization - Extractive Approach\n","\n","#(Slip No. 8 - Unique)\n","\n","import nltk\n","import re\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","from collections import defaultdict\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","text = \"\"\"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans using natural language. NLP techniques enable computers to process and understand human language, allowing applications such as speech recognition, machine translation, and sentiment analysis. The field has seen significant advancements due to deep learning and large-scale language models.\"\"\"\n","\n","# Clean text\n","clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","sentences = sent_tokenize(clean_text)\n","\n","# Calculate word frequency\n","stop_words = set(stopwords.words('english'))\n","word_freq = defaultdict(int)\n","for sentence in sentences:\n","    words = word_tokenize(sentence.lower())\n","    for word in words:\n","        if word not in stop_words:\n","            word_freq[word] += 1\n","\n","# Score sentences\n","sentence_scores = {i: sum(word_freq[word] for word in word_tokenize(sent.lower()) if word in word_freq) for i, sent in enumerate(sentences)}\n","\n","# Select top sentences\n","top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:2]\n","summary = \" \".join(sentences[i] for i in sorted(top_sentences))\n","\n","print(\"Extractive Summary:\\n\", summary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"chb9zpy-9D1b","executionInfo":{"status":"error","timestamp":1741625289593,"user_tz":-330,"elapsed":4475,"user":{"displayName":"Joel Chinta","userId":"00861839643618150718"}},"outputId":"b4bd6c0c-65fc-42c1-e463-b2de01bcd2e7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"error","ename":"LookupError","evalue":"\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-d20360f84ea5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Clean text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mclean_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^a-zA-Z\\s]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Calculate word frequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"]}]},{"cell_type":"code","source":["#6. Linear Regression for Fish Species Weight Prediction\n","\n","#(Using Fish Market Dataset from Kaggle)\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# Load the dataset\n","file_path = \"/mnt/data/file-UALYDjz2i1yWVGCGNFMdWL\"  # Update with your actual file path\n","df = pd.read_csv(file_path)\n","\n","# Display first few rows\n","print(\"Dataset Sample:\")\n","print(df.head())\n","\n","# Selecting features and target variable\n","X = df[['Length1', 'Length2', 'Length3', 'Height', 'Width']]  # Features\n","y = df['Weight']  # Target variable\n","\n","# Splitting dataset (70% training, 30% testing)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Train the Linear Regression model\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = model.predict(X_test)\n","\n","# Model evaluation\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Print results\n","print(\"\\nModel Coefficients:\")\n","print(f\"Intercept: {model.intercept_:.4f}\")\n","for feature, coef in zip(X.columns, model.coef_):\n","    print(f\"{feature}: {coef:.4f}\")\n","\n","print(\"\\nModel Performance:\")\n","print(f\"Mean Squared Error: {mse:.4f}\")\n","print(f\"R-squared Score: {r2:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":738},"id":"C38tnaPN9ReD","executionInfo":{"status":"error","timestamp":1741625372562,"user_tz":-330,"elapsed":401,"user":{"displayName":"Joel Chinta","userId":"00861839643618150718"}},"outputId":"3040004d-21b7-4584-c8d8-7cbefbded253"},"execution_count":7,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/mnt/data/file-UALYDjz2i1yWVGCGNFMdWL'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-92da10a4a892>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/mnt/data/file-UALYDjz2i1yWVGCGNFMdWL\"\u001b[0m  \u001b[0;31m# Update with your actual file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Display first few rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/file-UALYDjz2i1yWVGCGNFMdWL'"]}]},{"cell_type":"code","source":["#1Logistic Regression on the Iris Dataset\n","\n","#(Statistical Summary + Classification)\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn import datasets\n","\n","# Load the Iris dataset\n","iris = datasets.load_iris()\n","df = pd.DataFrame(iris.data, columns=iris.feature_names)\n","df['species'] = iris.target\n","\n","# Convert target numbers to species names\n","df['species'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n","\n","# Display basic statistical details\n","print(\"Basic Statistics:\\n\")\n","print(df.groupby('species').describe())\n","\n","# Prepare data for logistic regression\n","X = df.iloc[:, :-1]  # Features: Sepal/Petal lengths & widths\n","y = df['species']  # Target variable\n","\n","# Encode labels (setosa=0, versicolor=1, virginica=2)\n","encoder = LabelEncoder()\n","y_encoded = encoder.fit_transform(y)\n","\n","# Split into train and test sets (70% training, 30% testing)\n","X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n","\n","# Train Logistic Regression model\n","model = LogisticRegression(max_iter=200)\n","model.fit(X_train, y_train)\n","\n","# Predict species\n","y_pred = model.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"\\nModel Accuracy: {accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4AkbReGO9bDt","executionInfo":{"status":"ok","timestamp":1741625410484,"user_tz":-330,"elapsed":691,"user":{"displayName":"Joel Chinta","userId":"00861839643618150718"}},"outputId":"799b7443-0a1b-421b-ba34-98713735c9e7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Basic Statistics:\n","\n","           sepal length (cm)                                              \\\n","                       count   mean       std  min    25%  50%  75%  max   \n","species                                                                    \n","setosa                  50.0  5.006  0.352490  4.3  4.800  5.0  5.2  5.8   \n","versicolor              50.0  5.936  0.516171  4.9  5.600  5.9  6.3  7.0   \n","virginica               50.0  6.588  0.635880  4.9  6.225  6.5  6.9  7.9   \n","\n","           sepal width (cm)         ... petal length (cm)       \\\n","                      count   mean  ...               75%  max   \n","species                             ...                          \n","setosa                 50.0  3.428  ...             1.575  1.9   \n","versicolor             50.0  2.770  ...             4.600  5.1   \n","virginica              50.0  2.974  ...             5.875  6.9   \n","\n","           petal width (cm)                                            \n","                      count   mean       std  min  25%  50%  75%  max  \n","species                                                                \n","setosa                 50.0  0.246  0.105386  0.1  0.2  0.2  0.3  0.6  \n","versicolor             50.0  1.326  0.197753  1.0  1.2  1.3  1.5  1.8  \n","virginica              50.0  2.026  0.274650  1.4  1.8  2.0  2.3  2.5  \n","\n","[3 rows x 32 columns]\n","\n","Model Accuracy: 1.0000\n"]}]}]}